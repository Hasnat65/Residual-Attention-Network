{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Residual Attention Network\n",
    "\n",
    "- A stack of Attention Modules\n",
    "- Attention Modules have 2 branches\n",
    "    - Trunk Branch\n",
    "    - Soft Mask Branch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Input, Conv2D, MaxPool2D, UpSampling2D, Activation, AveragePooling2D, Flatten, Dense, Add, Multiply, BatchNormalization\n",
    "from tensorflow.keras.models import Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- https://keras.io/layers/merge/\n",
    "- https://keras.io/layers/convolutional/\n",
    "- https://keras.io/activations/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualAttentionNetwork():\n",
    "    \n",
    "    def __init__(self, input_shape, n_classes, p=1, t=2, r=1):\n",
    "        '''\n",
    "        Params: \n",
    "        - num attention modules\n",
    "        - p\n",
    "        - r\n",
    "        - t\n",
    "        \n",
    "        \n",
    "        Conv Layer\n",
    "        Max Pooling Layer\n",
    "        \n",
    "        Residual Unit\n",
    "        Attention Module\n",
    "        \n",
    "        Residual Unit\n",
    "        Attention Module\n",
    "        \n",
    "        Residual Unit\n",
    "        Attention Module\n",
    "\n",
    "        Residual Unit\n",
    "        \n",
    "        Average Pooling\n",
    "        \n",
    "        Flatten\n",
    "        \n",
    "        Dense Layer(s)\n",
    "        Output Dense Layer (Num. Classer, activation='softmax'))\n",
    "        '''\n",
    "        \n",
    "        # Initialize a Keras Tensor of input_shape\n",
    "        input_data = Input(shape=input_shape)\n",
    "        \n",
    "        # Initial Layers before Attention Module\n",
    "        conv_layer_1 = self.convolution_layer(input_data)\n",
    "        max_pool_layer_1 = self.max_pool_layer(conv_layer_1)\n",
    "\n",
    "        # Residual Unit then Attention Module #1\n",
    "        res_unit_1 = self.residual_unit(max_pool_layer_1)\n",
    "        att_mod_1 = self.attention_module(res_unit_1, p, t, r)\n",
    "\n",
    "        # Residual Unit then Attention Module #1\n",
    "        res_unit_2 = self.residual_unit(att_mod_1)\n",
    "        att_mod_2 = self.attention_module(res_unit_2, p, t, r)\n",
    "        \n",
    "        # Residual Unit then Attention Module #1\n",
    "        res_unit_3 = self.residual_unit(att_mod_2)\n",
    "        att_mod_3 = self.attention_module(res_unit_3, p, t, r)\n",
    "        \n",
    "        # Ending it all\n",
    "        res_unit_end_1 = self.residual_unit(att_mod_3)\n",
    "        res_unit_end_2 = self.residual_unit(res_unit_end_1)\n",
    "        res_unit_end_3 = self.residual_unit(res_unit_end_2)\n",
    "        res_unit_end_4 = self.residual_unit(res_unit_end_3)\n",
    "        \n",
    "        avg_pool_layer = self.avg_pool_layer(res_unit_end_4)\n",
    "        \n",
    "        flatten_op = Flatten()(avg_pool_layer)\n",
    "        \n",
    "        fully_connected_layers = Dense(n_classes, activation='softmax')(flatten_op)\n",
    "        \n",
    "        ran_model = Model(inputs=input_data, outputs=fully_connected_layers)\n",
    "        \n",
    "        return ran_model\n",
    "\n",
    "    def convolution_layer(self, input_x, filters=32, kernel_size=(5, 5), strides=(1, 1)):\n",
    "        \n",
    "        conv_op = Conv2D(filters=filters, \n",
    "                   kernel_size=kernel_size,\n",
    "                   strides=strides,\n",
    "                   padding='same') (input_x)\n",
    "        \n",
    "        batch_op = BatchNormalization() (conv_op)\n",
    "        \n",
    "        activation_op = Activation('relu') (batch_op) \n",
    "    \n",
    "        return activation_op\n",
    "    \n",
    "    def max_pool_layer(self, input_data, pool_size=(2, 2), strides=(2, 2)):\n",
    "        return MaxPool2D(pool_size=pool_size, \n",
    "                      strides=strides, \n",
    "                      padding='same') (input_data)\n",
    "    \n",
    "    def avg_pool_layer(self, input_data, pool_size=(2, 2), strides=(2, 2)):\n",
    "        return AveragePooling2D(pool_size=pool_size, \n",
    "                      strides=strides, \n",
    "                      padding='same') (input_data)\n",
    "    \n",
    "    def upsampling_layer(self, input_data, size=(2, 2), interpolation='bilinear'):\n",
    "        return UpSampling2D(size=size, \n",
    "                            interpolation=interpolation) (input_data)\n",
    "    \n",
    "    def residual_unit(self, input_x):\n",
    "        # Hold input_x here for later processing\n",
    "        skipped_x = input_x\n",
    "\n",
    "        # Layer 1\n",
    "        res_conv_1 = self.convolution_layer(input_x, filters=32)\n",
    "        \n",
    "        # Layer 2\n",
    "        res_conv_2 = self.convolution_layer(res_conv_1, filters=64)\n",
    "        \n",
    "        # Connecting Layer\n",
    "        output = self.connecting_residual_layer(input_x=res_conv_2, skipped_x=skipped_x)\n",
    "        \n",
    "        return output\n",
    "    \n",
    "    def connecting_residual_layer(self, input_x, skipped_x, filters=32, kernel_size=(5, 5), strides=(1, 1)):\n",
    "        # Connecting Layer\n",
    "        conv_op = Conv2D(filters=filters, \n",
    "                   kernel_size=kernel_size,\n",
    "                   strides=strides,\n",
    "                   padding='same') (input_x)\n",
    "        \n",
    "        batch_op = BatchNormalization() (conv_op)\n",
    "        \n",
    "        # Combine processed_x with input_x, aka skipped_x\n",
    "        add_op = Add()[batch_op, skipped_x]\n",
    "        \n",
    "        activation_op = Activation('relu') (add_op) \n",
    "    \n",
    "        return activation_op\n",
    "    \n",
    "    def attention_module(self, input_x, p, t, r):\n",
    "        \n",
    "        # Send input_x through #p residual_units\n",
    "        p_res_unit_op_1 = input_x\n",
    "        for _ in range(p):\n",
    "            p_res_unit_op = self.residual_unit(p_res_unit_op)\n",
    "        \n",
    "        # Perform Trunk Branch Operation\n",
    "        trunk_branch_op = self.trunk_branch(trunk_input_x=p_res_unit_op, t=t)\n",
    "        \n",
    "        # Perform Mask Branch Operation\n",
    "        mask_branch_op = self.mask_branch(mask_input_x=p_res_unit_op, r=r)\n",
    "        \n",
    "        # Perform Attention Residual Learning: Combine Trunk and Mask branch results\n",
    "        ar_learning_op = self.attention_residual_learning(mask_input=mask_branch_op, trunk_input=trunk_branch_op)\n",
    "        \n",
    "        # Send branch results through #p residual_units\n",
    "        p_res_unit_op_2 = ar_learning_op\n",
    "        for _ in range(p):\n",
    "            p_res_unit_op_2 = self.residual_unit(p_res_unit_op_2)\n",
    "        \n",
    "        return p_res_unit_op_2\n",
    "    \n",
    "    def trunk_branch(self, trunk_input_x, t):\n",
    "        #sequence of residual units\n",
    "        t_res_unit_op = trunk_input_x\n",
    "        for _ in range(t):\n",
    "            t_res_unit_op = self.residual_unit(t_res_unit_op)\n",
    "            \n",
    "        return t_res_unit_op\n",
    "\n",
    "    def mask_branch(self, mask_input_x, r, m=3):    \n",
    "        # r = num of residual units between adjacent pooling layers\n",
    "        # m = num max pooling / linear interpolations to do\n",
    "        \n",
    "        # Downsampling Step\n",
    "        downsampling = mask_input_x\n",
    "        \n",
    "        for _ in range(m):\n",
    "            downsampling = self.max_pool_layer(input_data=downsampling)\n",
    "            \n",
    "            # Perform residual units ops r times between adjacent pooling layers\n",
    "            for _ in range(r):\n",
    "                downsampling = self.residual_unit(input_x=downsampling)\n",
    "        \n",
    "        # Last pooling step before middle step\n",
    "        downsampling = self.max_pool_layer(input_data=downsampling)\n",
    "        \n",
    "        # Perform 2*r residual units steps before upsampling\n",
    "        middleware = downsampling\n",
    "        for _ in range(2*r):\n",
    "            middleware = self.residual_unit(input_x=middleware)\n",
    "        \n",
    "        # Upsampling Step\n",
    "        upsampling = middleware\n",
    "        \n",
    "        for _ in range(m):\n",
    "            upsampling = self.upsampling_layer(input_data=upsampling)\n",
    "            \n",
    "            # Perform residual units ops r times between adjacent pooling layers\n",
    "            for _ in range(r):\n",
    "                upsampling = self.residual_unit(input_x=upsampling)\n",
    "        \n",
    "        # Last interpolation step\n",
    "        upsampling = self.upsampling_layer(input_data=upsampling)\n",
    "        \n",
    "        conv1 = self.convolution_layer(input_x=upsampling, kernel_size=(1,1))\n",
    "        conv2 = self.convolution_layer(input_x=conv1, kernel_size=(1,1))\n",
    "        \n",
    "        sigmoid = Activation('sigmoid') (conv2)\n",
    "        \n",
    "        return sigmoid\n",
    "    \n",
    "    def attention_residual_learning(self, mask_input, trunk_input):\n",
    "        M = Add()[1, mask_input] # 1 + mask\n",
    "        return Multiply()[M, trunk_input] # M(x) * T(x)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
