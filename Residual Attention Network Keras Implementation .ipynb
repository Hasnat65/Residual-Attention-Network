{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Residual Attention Network\n",
    "https://arxiv.org/abs/1704.06904\n",
    "- A stack of Attention Modules\n",
    "- Attention Modules have 2 branches\n",
    "    - Trunk Branch\n",
    "    - Soft Mask Branch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helpful Links\n",
    "- https://towardsdatascience.com/residual-blocks-building-blocks-of-resnet-fd90ca15d6ec\n",
    "- https://towardsdatascience.com/understanding-and-coding-a-resnet-in-keras-446d7ff84d33\n",
    "- https://towardsdatascience.com/review-residual-attention-network-attention-aware-features-image-classification-7ae44c4f4b8\n",
    "- https://sebastianwallkoetter.wordpress.com/2018/04/08/layered-layers-residual-blocks-in-the-sequential-keras-api/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Input, Conv2D, Lambda, MaxPool2D, UpSampling2D, AveragePooling2D, ZeroPadding2D\n",
    "from tensorflow.keras.layers import Activation, Flatten, Dense, Add, Multiply, BatchNormalization\n",
    "\n",
    "from tensorflow.keras.models import Model\n",
    "import keras\n",
    "\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Residual Attention Network Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Todo: Make scalable/all-encompassing\n",
    "class ResidualAttentionNetwork():\n",
    "\n",
    "    def __init__(self, input_shape, n_classes, p=1, t=2, r=1):\n",
    "        '''\n",
    "        Params:\n",
    "        - num attention modules\n",
    "        - p\n",
    "        - r\n",
    "        - t\n",
    "\n",
    "\n",
    "        Conv Layer\n",
    "        Max Pooling Layer\n",
    "\n",
    "        Residual Unit\n",
    "        Attention Module\n",
    "\n",
    "        Residual Unit\n",
    "        Attention Module\n",
    "\n",
    "        Residual Unit\n",
    "        Attention Module\n",
    "\n",
    "        Residual Unit\n",
    "\n",
    "        Average Pooling\n",
    "\n",
    "        Flatten\n",
    "\n",
    "        Dense Layer(s)\n",
    "        Output Dense Layer (Num. Classer, activation='softmax'))\n",
    "        '''\n",
    "\n",
    "        # Initialize a Keras Tensor of input_shape\n",
    "        input_data = Input(shape=input_shape)\n",
    "        \n",
    "        # Initial Layers before Attention Module\n",
    "        \n",
    "        # Doing padding because I'm having trouble with img dims that are <= 28\n",
    "        if input_shape[0] <= 28 or input_shape[1] <= 28:\n",
    "            x_dim_inc = (32 - input_shape[0]) // 2\n",
    "            y_dim_inc = (32 - input_shape[1]) // 2\n",
    "            padded_input_data = ZeroPadding2D( (x_dim_inc,y_dim_inc) )(input_data)\n",
    "            conv_layer_1 = self.convolution_layer(conv_input_data=padded_input_data)\n",
    "        else:\n",
    "            conv_layer_1 = self.convolution_layer(conv_input_data=input_data)\n",
    "            \n",
    "        \n",
    "        \n",
    "        max_pool_layer_1 = self.max_pool_layer(conv_layer_1)\n",
    "\n",
    "        # Residual Unit then Attention Module #1\n",
    "        res_unit_1 = self.residual_unit(max_pool_layer_1)\n",
    "        \n",
    "        att_mod_1 = self.attention_module(res_unit_1, p, t, r)\n",
    "        \n",
    "        # Residual Unit then Attention Module #2\n",
    "        res_unit_2 = self.residual_unit(att_mod_1)\n",
    "        att_mod_2 = self.attention_module(res_unit_2, p, t, r)\n",
    "\n",
    "        # Residual Unit then Attention Module #3\n",
    "        res_unit_3 = self.residual_unit(att_mod_2)\n",
    "        att_mod_3 = self.attention_module(res_unit_3, p, t, r)\n",
    "\n",
    "        # Ending it all\n",
    "        res_unit_end_1 = self.residual_unit(att_mod_3)\n",
    "        res_unit_end_2 = self.residual_unit(res_unit_end_1)\n",
    "        res_unit_end_3 = self.residual_unit(res_unit_end_2)\n",
    "        res_unit_end_4 = self.residual_unit(res_unit_end_3)\n",
    "\n",
    "        # Avg Pooling\n",
    "        avg_pool_layer = self.avg_pool_layer(res_unit_end_4)\n",
    "\n",
    "        # Flatten the data\n",
    "        flatten_op = Flatten()(avg_pool_layer)\n",
    "\n",
    "        # FC Layer for prediction\n",
    "        fully_connected_layers = Dense(n_classes, activation='softmax')(flatten_op)\n",
    "\n",
    "        # Fully constructed model\n",
    "        self.model = Model(inputs=input_data, outputs=fully_connected_layers)\n",
    "\n",
    "    def convolution_layer(self, conv_input_data, filters=32, kernel_size=(3, 3), strides=(1, 1)):\n",
    "\n",
    "        conv_op = Conv2D(filters=filters,\n",
    "                         kernel_size=kernel_size,\n",
    "                         strides=strides,\n",
    "                         padding='same')(conv_input_data)\n",
    "\n",
    "        batch_op = BatchNormalization()(conv_op)\n",
    "\n",
    "        activation_op = Activation('relu')(batch_op)\n",
    "\n",
    "        return activation_op\n",
    "\n",
    "    def max_pool_layer(self, pool_input_data, pool_size=(2, 2), strides=(2, 2)):\n",
    "        return MaxPool2D(pool_size=pool_size,\n",
    "                         strides=strides,\n",
    "                         padding='same')(pool_input_data)\n",
    "\n",
    "    def avg_pool_layer(self, pool_input_data, pool_size=(2, 2), strides=(2, 2)):\n",
    "        return AveragePooling2D(pool_size=pool_size,\n",
    "                                strides=strides,\n",
    "                                padding='same')(pool_input_data)\n",
    "\n",
    "    def upsampling_layer(self, upsampling_input_data, size=(2, 2), interpolation='bilinear'):\n",
    "        return UpSampling2D(size=size,\n",
    "                            interpolation=interpolation)(upsampling_input_data)\n",
    "\n",
    "    def residual_unit(self, residual_input_data):\n",
    "        # Hold input_x here for later processing\n",
    "        skipped_x = residual_input_data\n",
    "\n",
    "        # Layer 1\n",
    "        res_conv_1 = self.convolution_layer(conv_input_data=residual_input_data, filters=32)\n",
    "\n",
    "        # Layer 2\n",
    "        res_conv_2 = self.convolution_layer(conv_input_data=res_conv_1, filters=64)\n",
    "\n",
    "        # Connecting Layer\n",
    "        output = self.connecting_residual_layer(conn_input_data=res_conv_2, skipped_x=skipped_x)\n",
    "\n",
    "        return output\n",
    "\n",
    "    def connecting_residual_layer(self, conn_input_data, skipped_x, filters=32, kernel_size=(5, 5), strides=(1, 1)):\n",
    "        # Connecting Layer\n",
    "        conv_op = Conv2D(filters=filters,\n",
    "                         kernel_size=kernel_size,\n",
    "                         strides=strides,\n",
    "                         padding='same')(conn_input_data)\n",
    "\n",
    "        batch_op = BatchNormalization()(conv_op)\n",
    "        \n",
    "        # Todo: \n",
    "            # Do some work if skipped_x.shape is not the same as batch_op.shape\n",
    "            # Gotta do the convolution + batch_norm work on skipped x\n",
    "\n",
    "        # Combine processed_x with skipped_x\n",
    "        add_op = Add()([batch_op, skipped_x])\n",
    "\n",
    "        activation_op = Activation('relu')(add_op)\n",
    "\n",
    "        return activation_op\n",
    "\n",
    "    def attention_module(self, attention_input_data, p, t, r):\n",
    "\n",
    "        # Send input_x through #p residual_units\n",
    "        p_res_unit_op_1 = attention_input_data\n",
    "        for i in range(p):\n",
    "            p_res_unit_op_1 = self.residual_unit(p_res_unit_op_1)\n",
    "\n",
    "        # Perform Trunk Branch Operation\n",
    "        trunk_branch_op = self.trunk_branch(trunk_input_data=p_res_unit_op_1, t=t)\n",
    "\n",
    "        # Perform Mask Branch Operation\n",
    "        mask_branch_op = self.mask_branch(mask_input_data=p_res_unit_op_1, r=r)\n",
    "\n",
    "        # Perform Attention Residual Learning: Combine Trunk and Mask branch results\n",
    "        ar_learning_op = self.attention_residual_learning(mask_input=mask_branch_op, trunk_input=trunk_branch_op)\n",
    "\n",
    "        # Send branch results through #p residual_units\n",
    "        p_res_unit_op_2 = ar_learning_op\n",
    "        for _ in range(p):\n",
    "            p_res_unit_op_2 = self.residual_unit(p_res_unit_op_2)\n",
    "\n",
    "        return p_res_unit_op_2\n",
    "\n",
    "    def trunk_branch(self, trunk_input_data, t):\n",
    "        # sequence of residual units\n",
    "        t_res_unit_op = trunk_input_data\n",
    "        for _ in range(t):\n",
    "            t_res_unit_op = self.residual_unit(t_res_unit_op)\n",
    "\n",
    "        return t_res_unit_op\n",
    "\n",
    "    def mask_branch(self, mask_input_data, r, m=3):\n",
    "        # r = num of residual units between adjacent pooling layers\n",
    "        # m = num max pooling / linear interpolations to do\n",
    "\n",
    "        # Downsampling Step Initialization - Top\n",
    "        downsampling = self.max_pool_layer(pool_input_data=mask_input_data)\n",
    "\n",
    "        # Perform residual units ops r times between adjacent pooling layers\n",
    "        for j in range(r):\n",
    "            downsampling = self.residual_unit(residual_input_data=downsampling)\n",
    "\n",
    "        # Last pooling step before middle step - Bottom\n",
    "        downsampling = self.max_pool_layer(pool_input_data=downsampling)\n",
    "\n",
    "        # Middle Residuals - Perform 2*r residual units steps before upsampling\n",
    "        middleware = downsampling\n",
    "        for _ in range(2 * r):\n",
    "            middleware = self.residual_unit(residual_input_data=middleware)\n",
    "\n",
    "        # Upsampling Step Initialization - Top\n",
    "        upsampling = self.upsampling_layer(upsampling_input_data=middleware)\n",
    "\n",
    "        # Perform residual units ops r times between adjacent pooling layers\n",
    "        for j in range(r):\n",
    "            upsampling = self.residual_unit(residual_input_data=upsampling)\n",
    "\n",
    "        # Last interpolation step - Bottom\n",
    "        upsampling = self.upsampling_layer(upsampling_input_data=upsampling)\n",
    "\n",
    "        conv1 = self.convolution_layer(conv_input_data=upsampling, kernel_size=(1, 1))\n",
    "        conv2 = self.convolution_layer(conv_input_data=conv1, kernel_size=(1, 1))\n",
    "\n",
    "        sigmoid = Activation('sigmoid')(conv2)\n",
    "\n",
    "        return sigmoid\n",
    "\n",
    "    def attention_residual_learning(self, mask_input, trunk_input):\n",
    "        # https://stackoverflow.com/a/53361303/9221241\n",
    "        m = Lambda(lambda x: 1 + x)(mask_input) # 1 + mask\n",
    "        \n",
    "        # https://www.tensorflow.org/api_docs/python/tf/pad\n",
    "        # https://stackoverflow.com/questions/43928642/how-does-tensorflow-pad-work\n",
    "        # https://stackoverflow.com/questions/34141430/tensorflow-tensor-reshape-and-pad-with-zeros\n",
    "        # if m.shape != trunk_input.shape:\n",
    "        #    print(max(m.shape[1], trunk_input.shape[1]),max(m.shape[2],trunk_input.shape[2]))\n",
    "            \n",
    "        return Multiply()([m, trunk_input]) # M(x) * T(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Execution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### https://keras.io/examples/mnist_cnn/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ResidualAttentionNetwork(input_shape,num_classes).model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=keras.losses.categorical_crossentropy,\n",
    "              optimizer=\"adam\",\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit_generator(generator=train_generator,\n",
    "                    steps_per_epoch=STEP_SIZE_TRAIN,\n",
    "                    validation_data=validation_generator,\n",
    "                    validation_steps=STEP_SIZE_VALID,\n",
    "                    workers=8,\n",
    "                    use_multiprocessing=True,\n",
    "                    epochs=epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(12, 12))\n",
    "ax1.plot(history.history['loss'], color='b', label=\"Training loss\")\n",
    "ax1.plot(history.history['val_loss'], color='r', label=\"validation loss\")\n",
    "ax1.set_xticks(np.arange(1, epochs, 1))\n",
    "ax1.set_yticks(np.arange(0, 1, 0.1))\n",
    "\n",
    "ax2.plot(history.history['acc'], color='b', label=\"Training accuracy\")\n",
    "ax2.plot(history.history['val_acc'], color='r',label=\"Validation accuracy\")\n",
    "ax2.set_xticks(np.arange(1, epochs, 1))\n",
    "\n",
    "legend = plt.legend(loc='best', shadow=True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = model.evaluate(x_test, y_test, verbose=0)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_generator.classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.fit(x_train, y_train,\n",
    "                    batch_size=batch_size,\n",
    "                    epochs=epochs,\n",
    "\n",
    "                    validation_data=(x_test, y_test))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
