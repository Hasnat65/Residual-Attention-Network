{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Residual Attention Network\n",
    "\n",
    "- A stack of Attention Modules\n",
    "- Attention Modules have 2 branches\n",
    "    - Trunk Branch\n",
    "    - Soft Mask Branch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Input, Conv2D, Lambda, MaxPool2D, UpSampling2D, AveragePooling2D\n",
    "from tensorflow.keras.layers import Activation, Flatten, Dense, Add, Multiply, BatchNormalization\n",
    "\n",
    "from tensorflow.keras.models import Model\n",
    "import keras\n",
    "\n",
    "import os\n",
    "from keras.datasets import cifar10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Todo: Make scalable/all-encompassing\n",
    "class ResidualAttentionNetwork():\n",
    "\n",
    "    def __init__(self, input_shape, n_classes, p=1, t=2, r=1):\n",
    "        '''\n",
    "        Params:\n",
    "        - num attention modules\n",
    "        - p\n",
    "        - r\n",
    "        - t\n",
    "\n",
    "\n",
    "        Conv Layer\n",
    "        Max Pooling Layer\n",
    "\n",
    "        Residual Unit\n",
    "        Attention Module\n",
    "\n",
    "        Residual Unit\n",
    "        Attention Module\n",
    "\n",
    "        Residual Unit\n",
    "        Attention Module\n",
    "\n",
    "        Residual Unit\n",
    "\n",
    "        Average Pooling\n",
    "\n",
    "        Flatten\n",
    "\n",
    "        Dense Layer(s)\n",
    "        Output Dense Layer (Num. Classer, activation='softmax'))\n",
    "        '''\n",
    "\n",
    "        # Initialize a Keras Tensor of input_shape\n",
    "        input_data = Input(shape=input_shape)\n",
    "        \n",
    "        # Initial Layers before Attention Module\n",
    "        conv_layer_1 = self.convolution_layer(conv_input_data=input_data)\n",
    "        \n",
    "        max_pool_layer_1 = self.max_pool_layer(conv_layer_1)\n",
    "\n",
    "        # Residual Unit then Attention Module #1\n",
    "        res_unit_1 = self.residual_unit(max_pool_layer_1)\n",
    "        \n",
    "        att_mod_1 = self.attention_module(res_unit_1, p, t, r)\n",
    "        \n",
    "        # Residual Unit then Attention Module #2\n",
    "        res_unit_2 = self.residual_unit(att_mod_1)\n",
    "        att_mod_2 = self.attention_module(res_unit_2, p, t, r)\n",
    "\n",
    "        # Residual Unit then Attention Module #3\n",
    "        res_unit_3 = self.residual_unit(att_mod_2)\n",
    "        att_mod_3 = self.attention_module(res_unit_3, p, t, r)\n",
    "\n",
    "        # Ending it all\n",
    "        res_unit_end_1 = self.residual_unit(att_mod_3)\n",
    "        res_unit_end_2 = self.residual_unit(res_unit_end_1)\n",
    "        res_unit_end_3 = self.residual_unit(res_unit_end_2)\n",
    "        res_unit_end_4 = self.residual_unit(res_unit_end_3)\n",
    "\n",
    "        # Avg Pooling\n",
    "        avg_pool_layer = self.avg_pool_layer(res_unit_end_4)\n",
    "\n",
    "        # Flatten the data\n",
    "        flatten_op = Flatten()(avg_pool_layer)\n",
    "\n",
    "        # FC Layer for prediction\n",
    "        fully_connected_layers = Dense(n_classes, activation='softmax')(flatten_op)\n",
    "\n",
    "        # Fully constructed model\n",
    "        self.model = Model(inputs=input_data, outputs=fully_connected_layers)\n",
    "\n",
    "    def convolution_layer(self, conv_input_data, filters=32, kernel_size=(5, 5), strides=(1, 1)):\n",
    "\n",
    "        conv_op = Conv2D(filters=filters,\n",
    "                         kernel_size=kernel_size,\n",
    "                         strides=strides,\n",
    "                         padding='same')(conv_input_data)\n",
    "\n",
    "        batch_op = BatchNormalization()(conv_op)\n",
    "\n",
    "        activation_op = Activation('relu')(batch_op)\n",
    "\n",
    "        return activation_op\n",
    "\n",
    "    def max_pool_layer(self, pool_input_data, pool_size=(2, 2), strides=(2, 2)):\n",
    "        return MaxPool2D(pool_size=pool_size,\n",
    "                         strides=strides,\n",
    "                         padding='same')(pool_input_data)\n",
    "\n",
    "    def avg_pool_layer(self, pool_input_data, pool_size=(2, 2), strides=(2, 2)):\n",
    "        return AveragePooling2D(pool_size=pool_size,\n",
    "                                strides=strides,\n",
    "                                padding='same')(pool_input_data)\n",
    "\n",
    "    def upsampling_layer(self, upsampling_input_data, size=(2, 2), interpolation='bilinear'):\n",
    "        return UpSampling2D(size=size,\n",
    "                            interpolation=interpolation)(upsampling_input_data)\n",
    "\n",
    "    def residual_unit(self, residual_input_data):\n",
    "        # Hold input_x here for later processing\n",
    "        skipped_x = residual_input_data\n",
    "\n",
    "        # Layer 1\n",
    "        res_conv_1 = self.convolution_layer(conv_input_data=residual_input_data, filters=32)\n",
    "\n",
    "        # Layer 2\n",
    "        res_conv_2 = self.convolution_layer(conv_input_data=res_conv_1, filters=64)\n",
    "\n",
    "        # Connecting Layer\n",
    "        output = self.connecting_residual_layer(conn_input_data=res_conv_2, skipped_x=skipped_x)\n",
    "\n",
    "        return output\n",
    "\n",
    "    def connecting_residual_layer(self, conn_input_data, skipped_x, filters=32, kernel_size=(5, 5), strides=(1, 1)):\n",
    "        # Connecting Layer\n",
    "        conv_op = Conv2D(filters=filters,\n",
    "                         kernel_size=kernel_size,\n",
    "                         strides=strides,\n",
    "                         padding='same')(conn_input_data)\n",
    "\n",
    "        batch_op = BatchNormalization()(conv_op)\n",
    "        \n",
    "        # Todo: \n",
    "            # Do some work if skipped_x.shape is not the same as batch_op.shape\n",
    "            # Gotta do the convolution + batch_norm work on skipped x\n",
    "\n",
    "        # Combine processed_x with skipped_x\n",
    "        add_op = Add()([batch_op, skipped_x])\n",
    "\n",
    "        activation_op = Activation('relu')(add_op)\n",
    "\n",
    "        return activation_op\n",
    "\n",
    "    def attention_module(self, attention_input_data, p, t, r):\n",
    "\n",
    "        # Send input_x through #p residual_units\n",
    "        p_res_unit_op_1 = attention_input_data\n",
    "        for i in range(p):\n",
    "            p_res_unit_op_1 = self.residual_unit(p_res_unit_op_1)\n",
    "\n",
    "        # Perform Trunk Branch Operation\n",
    "        trunk_branch_op = self.trunk_branch(trunk_input_data=p_res_unit_op_1, t=t)\n",
    "\n",
    "        # Perform Mask Branch Operation\n",
    "        mask_branch_op = self.mask_branch(mask_input_data=p_res_unit_op_1, r=r)\n",
    "\n",
    "        # Perform Attention Residual Learning: Combine Trunk and Mask branch results\n",
    "        ar_learning_op = self.attention_residual_learning(mask_input=mask_branch_op, trunk_input=trunk_branch_op)\n",
    "\n",
    "        # Send branch results through #p residual_units\n",
    "        p_res_unit_op_2 = ar_learning_op\n",
    "        for _ in range(p):\n",
    "            p_res_unit_op_2 = self.residual_unit(p_res_unit_op_2)\n",
    "\n",
    "        return p_res_unit_op_2\n",
    "\n",
    "    def trunk_branch(self, trunk_input_data, t):\n",
    "        # sequence of residual units\n",
    "        t_res_unit_op = trunk_input_data\n",
    "        for _ in range(t):\n",
    "            t_res_unit_op = self.residual_unit(t_res_unit_op)\n",
    "\n",
    "        return t_res_unit_op\n",
    "\n",
    "    def mask_branch(self, mask_input_data, r, m=3):\n",
    "        # r = num of residual units between adjacent pooling layers\n",
    "        # m = num max pooling / linear interpolations to do\n",
    "\n",
    "        # Downsampling Step Initialization - Top\n",
    "        downsampling = self.max_pool_layer(pool_input_data=mask_input_data)\n",
    "\n",
    "        # Perform residual units ops r times between adjacent pooling layers\n",
    "        for j in range(r):\n",
    "            downsampling = self.residual_unit(residual_input_data=downsampling)\n",
    "\n",
    "        # Last pooling step before middle step - Bottom\n",
    "        downsampling = self.max_pool_layer(pool_input_data=downsampling)\n",
    "\n",
    "        # Middle Residuals - Perform 2*r residual units steps before upsampling\n",
    "        middleware = downsampling\n",
    "        for _ in range(2 * r):\n",
    "            middleware = self.residual_unit(residual_input_data=middleware)\n",
    "\n",
    "        # Upsampling Step Initialization - Top\n",
    "        upsampling = self.upsampling_layer(upsampling_input_data=middleware)\n",
    "\n",
    "        # Perform residual units ops r times between adjacent pooling layers\n",
    "        for j in range(r):\n",
    "            upsampling = self.residual_unit(residual_input_data=upsampling)\n",
    "\n",
    "        # Last interpolation step - Bottom\n",
    "        upsampling = self.upsampling_layer(upsampling_input_data=upsampling)\n",
    "\n",
    "        conv1 = self.convolution_layer(conv_input_data=upsampling, kernel_size=(1, 1))\n",
    "        conv2 = self.convolution_layer(conv_input_data=conv1, kernel_size=(1, 1))\n",
    "\n",
    "        sigmoid = Activation('sigmoid')(conv2)\n",
    "\n",
    "        return sigmoid\n",
    "\n",
    "    def attention_residual_learning(self, mask_input, trunk_input):\n",
    "        # https://stackoverflow.com/a/53361303/9221241\n",
    "        m = Lambda(lambda x: 1 + x)(mask_input) # 1 + mask\n",
    "        \n",
    "        # https://www.tensorflow.org/api_docs/python/tf/pad\n",
    "        # https://stackoverflow.com/questions/43928642/how-does-tensorflow-pad-work\n",
    "        # https://stackoverflow.com/questions/34141430/tensorflow-tensor-reshape-and-pad-with-zeros\n",
    "        # if m.shape != trunk_input.shape:\n",
    "        #    print(max(m.shape[1], trunk_input.shape[1]),max(m.shape[2],trunk_input.shape[2]))\n",
    "            \n",
    "        return Multiply()([m, trunk_input]) # M(x) * T(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://keras.io/examples/cifar10_cnn/\n",
    "batch_size = 32\n",
    "num_classes = 10\n",
    "epochs = 100\n",
    "data_augmentation = True\n",
    "num_predictions = 20\n",
    "save_dir = os.path.join(os.getcwd(), 'saved_models')\n",
    "model_name = 'keras_cifar10_trained_model.h5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape: (50000, 32, 32, 3)\n",
      "50000 train samples\n",
      "10000 test samples\n"
     ]
    }
   ],
   "source": [
    "# The data, split between train and test sets:\n",
    "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
    "print('x_train shape:', x_train.shape)\n",
    "print(x_train.shape[0], 'train samples')\n",
    "print(x_test.shape[0], 'test samples')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert class vectors to binary class matrices.\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = x_train.shape[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "ran_model = ResidualAttentionNetwork(input_shape=input_shape, n_classes=num_classes).model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "ran_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "x_train /= 255\n",
    "x_test /= 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 50000 samples, validate on 10000 samples\n",
      "WARNING:tensorflow:From /anaconda3/envs/MachineLearning/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Epoch 1/100\n",
      " 4736/50000 [=>............................] - ETA: 1:11:56 - loss: 13.3113 - acc: 0.1626"
     ]
    }
   ],
   "source": [
    "ran_model.fit(x_train, y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          validation_data=(x_test, y_test),\n",
    "          shuffle=True,    \n",
    "          workers=6,\n",
    "          use_multiprocessing=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
